---
typora-copy-images-to: \images

---

# Chapter 6 Temporal-Difference Learning

时序差分学习(temporal-difference learning, TD learning)完美结合了蒙特卡洛算法和动态规划算法的优点。像蒙特卡洛算法一样，TD算法可以在无环境模型的情况下，直接根据样本学习；像动态规划算法一样，TD算法可以基于部分已学习到的估计更新当前估计，不需要等到过程到达终止状态。

## TD Prediction

TD和蒙特卡洛算法都能够利用样本来解决预测问题。不同的是，蒙特卡洛算法必须等到整个周期结束后，才能得到回报$G_t$。每次访问的蒙特卡洛算法(every-visit Monte Carlo)是:

$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)] \tag{6.1}$$

这里$G_t$表示实际的回报，常数$\alpha$表示步长参数。如上所述，蒙特卡洛算法必须要等到整个周期结束后，才能决定期望$V(S_t)$的增加量。而TD算法只需要等到下一个时间片**（这里我理解为下一个状态）**即可。在$t+1$时刻，我们使用获得的奖励$R_{t+1}$和估计值$V(S_{t+1})$，就能计算出$V(S_t)$的目标值。最简单的TD更新算法如下：

$V(S_t) \gets V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \tag{6.2}$

在蒙特卡洛算法中，目标值是$G_t$；TD算法中，目标值是$R_{t+1} + \gamma V(S_{t + 1})$。式6.2被称为TD(0)，或者是单步TD(one-step TD)。算法如下：

![1506423198752](images/1506423198752.png)

因为TD(0)的更新与部分已存在的估计值有关，这很容易让我们联想到第三章的引导方法(bootstrapping methods)，像

$$\begin{align} v_{\pi}(s) & \approx E_{\pi}[Gt| S_t = s] \tag{6.3} \\ & = E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] \tag{from(3.3)}  \\ & = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]  \tag{6.4}\end{align}$$

蒙特卡洛算法的目标值是估计值的原因是我们无法知道式6.3的期望值，我们只能用样本的返回值来代替真正的期望值。动态规划算法的目标值是估计值，原因不在于期望值（我们可以假设期望值可以从环境中获得），原因在于$v_{\pi}(S_{t+1})$无法求出，只能用$V({S_{t+1})}$来代替。TD算法的目标值是估计值的原因有两个：一个是它对式6.4的期望值进行采样；另一个是它用当前的估计值$V$代替$v_{\pi}$。

![the backup diagram for tabular TD(0)](images/the backup diagram for tabular TD(0).jpg)

上图是TD(0)的演示图。我们把TD和蒙特卡洛算法的更新看作是样本回归(sample backup)。这两种方法都使用后续状态的估计值和本次的奖励，来计算回归值(backed_up value)，然后使用回归值对原始值进行更新。样本回归与DP算法的全回归的不同之处在于：DP算法依靠单一的样本序列而不是后续状态的完整分布。

现在，我们引入TD误差(TD error)这个概念，公式是:

$$\delta_t \approx R_{t+1} + \gamma V(S_{t+1} - V(S_t)) \tag{6.5}$$

$\delta_t$是$t$时刻的误差。

如果在整个周期中，数组$V$不在更新，蒙特卡洛算法的误差可以写成TD误差的和的形式：

$$\begin{align} G_t - V(S_t) & = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \tag{from (3.3)}\\ & = \delta_t + \gamma (G_{t+1} - V(S_{t+1}) ) \\ & = \delta_t + \gamma \delta_{t+1} + \dots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 - 0) \\ & = \sum_{k = t}^{T-1}\gamma^{k-t}\delta_{k} \tag{(6.6)} \end{align}$$

### Exercise 6.1

问：如果$V$在周期中变化，式6.6将是一个近似值。双方之间有什么区别？设$V_t$表示在t时刻的状态值数组，写出蒙特卡洛算法误差的表示式。

答：

pass

### Driving Home

每天你需要估计一下到家需要多长时间。你注意到，花费的时间与天气、周几等因素可能是相关的。某一天是周五，你在下午六点离开办公室，此时你估计可能要三十分钟到家。你到停车场时已经6：05了，这时你注意到可能要下雨了，你估计回家要花费三十五分钟（总计四十分钟）。接下来，你花了十五分钟在高速路上。路况很好，此时你估计你还要十五分钟才能到家。不幸的是，你被堵在一辆行驶很慢的大货车的后面。一直到6：40，你才逃出来。后来你又花了三分钟到家。状态、时间、预测的序列如下表所示：

|            State            | Elapsed Time | Predicted Time to Go | Predicted Total Time |
| :-------------------------: | :----------: | :------------------: | :------------------: |
| leaving office, Friday at 6 |      0       |          30          |          30          |
|     reach car, raining      |      5       |          35          |          40          |
|       exiting highway       |      20      |          15          |          35          |
|  2ndary road, behind truck  |      30      |          10          |          40          |
|    entering home street     |      40      |          3           |          43          |
|         arrive home         |      43      |          0           |          43          |

在本例中，奖励是到下一个状态花费的时间，$\gamma = 1$。状态的值是从此刻开始到回家，花费时间的期望。

![1506512439692](images/1506512439692.png)

左图是蒙特卡洛算法如何预测总时间的示意图。右侧是TD(0)算法。

在本例中，蒙特卡洛算法预计的时间可能与实际的时间有很大的偏差。例如，你认为你只需要15分钟就能离开高速路，实际上你花了23分钟。假设$\alpha = 0.5$，那么我们需要在预计的时间上再加上4分钟（0.5 * （23 - 15））。这个更新幅度太大了，不符合实际。

再假设，你被堵在了半路上。实际上，我们不需要到家（整个周期完成），我们就知道应该上调预期。但根据蒙特卡洛算法，只有到家之后，你才能调整前面的估计值。

但根据TD算法，每一个估计都应该像下一次状态的估计值趋近。在最初的例子中，随着进程的推进，成比例的根据误差对预期进行更新，这被称为预测的时序误差(temporal differences in predictions)。

## Advantages of TD Prediction Methods

TD算法根据部分已知的估计值来估计当前的值。因为TD算法可以直接使用样本进行训练，TD算法不需要环境给出奖励和下个状态的分布律，所以TD算法要优于动态规划算法。

对于蒙特卡洛算法，它必须要等到整个流程结束后，才能得到返回值。而TD算法只需要等到下一个序列就可以知道当前值。假设，某个流程的序列很长或者该任务是一个连续性任务，那么蒙特卡洛算法就会收敛的很慢甚至不能使用。此外，蒙特卡洛算法还要忽略或者可能不重视某些样本。TD算法就没有以上的问题，TD算法从每次状态转移中获得的学习与后续行为的选择无关。

TD算法一定能够收敛到正确值吗？是的，完全可以。对于固定策略$\pi$，步长参数不变且足够小，或者满足随机估计条件($\sum_{n= 1} ^{\infty} \alpha_n (a)  = \infty \ \ \ \ and \ \ \ \ \sum_{n=1}^{\infty} \alpha^2_{n} (a) < \infty$)，TD(0)就能够保证收敛到$v_{\pi}$。

既然TD算法和蒙特卡洛算法都能够收敛到正确值，那么哪种方法收敛的更快？哪种方法更能适应较少的数据？现在还没有一个准确的答案。

### Exercise 6.2

问：

答：

### Exercise 6.3

问：

答：

### Random Walk

现在，我们来比较一下TD(0)算法和蒙特卡洛算法在一个较小序列的马尔科夫决策过程的预测效果。开始状态是C（这个流程的中心），可选行为是等概率的向左或向右。流程将在agent到达最左边或最右边结束。如果结束时agent在最右边，奖励为+1，其它为0。从A到E的真实值是1/6，2/6，3/6，4/6，5/6。下图是流程示意图及实验结果分析。

![1506600338178](images/1506600338178.png)

左图说明TD(0)学习的次数越多，估计值就越接近真实值。基于对多个片段序列的平均，右图表示TD(0)算法和蒙特卡洛算法在预测方面的平均误差。实验中，变量是学习率$\alpha$。程序初始化时，把所有状态的价值设为0.5。从右图我们可以发现，TD算法始终优于蒙特卡洛算法。

### Exercise 6.4

问：

答：

### Exercise 6.5

问：

答：

### Exercise 6.6

问：

答：

## Optimality of TD(0)

假设我们现在只有有限个样本，例如10个片段或者100个序列。这样的情况下，最好的方式就是不停的利用现有的样本迭代，直到收敛。根据式6.1或6.2，价值函数在每次访问序列t被计算，且只被计算一次（通过求所有增加量的和）。然后所有可使用的样本再次被新的价值函数处理，产生新的全局增量，这样往复循环。因为我们在每次处理完一批训练数据后才更新，所以我们称这样的更新方式为批更新（batch updating）。

在批更新的方式下，TD(0)收敛到一个无关步长参数$\alpha$（$\alpha$要足够的小）的值。蒙特卡洛算法也会收敛到一个值，但这个值与TD(0)收敛到的值不等。在正常的更新方式下，两种方法收敛到的值应该相等，虽然它们有分离的迹象。

### Random walk under batch updating

156页
